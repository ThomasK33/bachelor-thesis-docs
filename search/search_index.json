{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bachelor Thesis","text":"","tags":["Kubernetes","thesis","uni"]},{"location":"#goal","title":"Goal","text":"<p>The goal of this Bachelor thesis is the creation of a network-topology-aware scheduler for Kubernetes.</p>","tags":["Kubernetes","thesis","uni"]},{"location":"#comparable-work","title":"Comparable Work","text":"","tags":["Kubernetes","thesis","uni"]},{"location":"#topology-aware-scheduling","title":"Topology Aware Scheduling","text":"<p>In 2020 a general-purpose Topology Aware Scheduling algorithm was implemented in the Kubernetes Scheduler.</p> <p>The original use case focuses on implementing NUMA-aware scheduling and introducing generalized Custom Resources to provide capacity information to the scheduler.</p>","tags":["Kubernetes","thesis","uni"]},{"location":"#state-of-the-art","title":"State of the Art","text":"<p>Currently, scheduling is available with extended resources. In upcoming Kubernetes versions, which will stabilize Device Plugins, using extended resources will become a viable scheduling mechanism for, e.g., FPGA devices, NICs, and GPUs. Yet, many things could be improved regarding the nature of network bandwidth, which can burst at most major cloud providers.</p> <p>Thus creating a specific scheduler plugin that can incorporate particular network bandwidth characteristics into its ratings is desirable. For example, pods managed by a Job or CronJob controller can still be placed onto nodes at their baseline, as those pods will have a start and an ending, thus enabling the option to use network burst without impacting the running pods in any way.</p>","tags":["Kubernetes","thesis","uni"]},{"location":"#target-implementation","title":"Target Implementation","text":"<p>Subject to change if needed.</p> <ol> <li>Define annotations to be added to nodes and pods to define networking bandwidth limits. --&gt; see Network Topology Aware Scheduler</li> <li>A mutating webhook that overrides the <code>schedulerName</code> field in a pod's object and forces it to use a specific scheduler. --&gt; see Network Bandwidth Annotation Manager</li> <li>Then extend the kube-scheduler to use the annotations, extending the scheduling algorithm while drawing inspiration from the Topology Aware Scheduling already present in the kube-scheduler.</li> <li>The extended scheduler can run in parallel, as one can configure multiple schedulers in a cluster.</li> </ol> <p>TODO: - Create custom metrics for the Metrics Server containing a node's maximum network bandwidth and, optionally, current bandwidth usage.</p>","tags":["Kubernetes","thesis","uni"]},{"location":"#resources","title":"Resources","text":"<ul> <li>https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/</li> <li>https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#extended-resources</li> </ul>","tags":["Kubernetes","thesis","uni"]},{"location":"Glossary/tags/","title":"Tags","text":""},{"location":"Glossary/tags/#aws","title":"AWS","text":"<ul> <li>Network Bandwidth Limits</li> </ul>"},{"location":"Glossary/tags/#cncf","title":"CNCF","text":"<ul> <li>Cert-manager</li> </ul>"},{"location":"Glossary/tags/#cni","title":"CNI","text":"<ul> <li>CNI Bandwidth Limiting</li> <li>How Kubernetes Networking Works</li> </ul>"},{"location":"Glossary/tags/#cri","title":"CRI","text":"<ul> <li>Runtime Metrics</li> </ul>"},{"location":"Glossary/tags/#cilium","title":"Cilium","text":"<ul> <li>CNI Bandwidth Limiting</li> </ul>"},{"location":"Glossary/tags/#ec2","title":"EC2","text":"<ul> <li>Network Bandwidth Limits</li> </ul>"},{"location":"Glossary/tags/#kubernetes","title":"Kubernetes","text":"<ul> <li>Bachelor Thesis</li> <li>Custom Resources</li> <li>Metrics Server</li> <li>Mutating Webhooks</li> <li>Cert-manager</li> <li>k3d</li> <li>CNI Bandwidth Limiting</li> <li>How Kubernetes Networking Works</li> <li>Runtime Metrics</li> <li>Kubernetes Scheduler</li> <li>Topology Aware Scheduling</li> <li>Network Topology Aware Scheduler</li> <li>Network Bandwidth Annotation Manager</li> <li>Scheduling Using Extended Resources</li> </ul>"},{"location":"Glossary/tags/#autoscaling","title":"autoscaling","text":"<ul> <li>Metrics Server</li> </ul>"},{"location":"Glossary/tags/#certificates","title":"certificates","text":"<ul> <li>Cert-manager</li> </ul>"},{"location":"Glossary/tags/#development-environment","title":"development-environment","text":"<ul> <li>k3d</li> </ul>"},{"location":"Glossary/tags/#extensible-admission-controllers","title":"extensible-admission-controllers","text":"<ul> <li>Mutating Webhooks</li> <li>Network Bandwidth Annotation Manager</li> </ul>"},{"location":"Glossary/tags/#kube-scheduler","title":"kube-scheduler","text":"<ul> <li>Kubernetes Scheduler</li> <li>Topology Aware Scheduling</li> <li>Network Topology Aware Scheduler</li> <li>Scheduling Using Extended Resources</li> </ul>"},{"location":"Glossary/tags/#metrics","title":"metrics","text":"<ul> <li>Metrics Server</li> <li>Runtime Metrics</li> </ul>"},{"location":"Glossary/tags/#networking","title":"networking","text":"<ul> <li>Network Bandwidth Limits</li> <li>How Kubernetes Networking Works</li> </ul>"},{"location":"Glossary/tags/#scheduler","title":"scheduler","text":"<ul> <li>Kubernetes Scheduler</li> <li>Network Topology Aware Scheduler</li> <li>Scheduling Using Extended Resources</li> </ul>"},{"location":"Glossary/tags/#thesis","title":"thesis","text":"<ul> <li>Bachelor Thesis</li> </ul>"},{"location":"Glossary/tags/#uni","title":"uni","text":"<ul> <li>Bachelor Thesis</li> </ul>"},{"location":"Glossary/tags/#x509","title":"x509","text":"<ul> <li>Cert-manager</li> </ul>"},{"location":"Glossary/AWS/EC2/Network%20Bandwidth%20Limits/","title":"Network Bandwidth Limits","text":"","tags":["AWS","EC2","networking"]},{"location":"Glossary/AWS/EC2/Network%20Bandwidth%20Limits/#references","title":"References","text":"<ul> <li>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html</li> <li>https://aws.amazon.com/premiumsupport/knowledge-center/ec2-instance-exceeding-network-limits/</li> <li>https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/compute-optimized-instances.html#compute-network-performance</li> </ul>","tags":["AWS","EC2","networking"]},{"location":"Glossary/Kubernetes/Custom%20Resources/","title":"Custom Resources","text":"","tags":["Kubernetes"]},{"location":"Glossary/Kubernetes/Custom%20Resources/#resources","title":"Resources","text":"<ul> <li>https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/</li> </ul>","tags":["Kubernetes"]},{"location":"Glossary/Kubernetes/Metrics%20Server/","title":"Metrics Server","text":"<p>Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines.</p> <p>The metrics Server collects resource metrics from Kubelets and exposes them in Kubernetes apiserver through\u00a0Metrics API\u00a0for use by\u00a0Horizontal Pod Autoscaler\u00a0and\u00a0Vertical Pod Autoscaler. Metrics API can also be accessed by\u00a0<code>kubectl top</code>, making it easier to debug autoscaling pipelines.1</p> <p>For Kubernetes, the\u00a0Metrics API\u00a0offers a basic set of metrics to support automatic scaling and similar use cases. This API provides information about node and pod resource usage, including CPU and memory metrics. Suppose you deploy the Metrics API into your cluster. In that case, clients of the Kubernetes API can then query for this information, and you can use Kubernetes' access control mechanisms to manage permissions to do so.2</p>","tags":["Kubernetes","metrics","autoscaling"]},{"location":"Glossary/Kubernetes/Metrics%20Server/#custom-metrics","title":"Custom Metrics","text":"<ul> <li>https://github.com/kubernetes-sigs/custom-metrics-apiserver</li> </ul>","tags":["Kubernetes","metrics","autoscaling"]},{"location":"Glossary/Kubernetes/Metrics%20Server/#installation","title":"Installation","text":"<pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n</code></pre> <ol> <li> <p>https://github.com/kubernetes-sigs/metrics-server \u21a9</p> </li> <li> <p>https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/ \u21a9</p> </li> </ol>","tags":["Kubernetes","metrics","autoscaling"]},{"location":"Glossary/Kubernetes/Mutating%20Webhooks/","title":"Mutating Webhooks","text":"","tags":["Kubernetes","extensible-admission-controllers"]},{"location":"Glossary/Kubernetes/Mutating%20Webhooks/#references","title":"References","text":"<ul> <li>https://github.com/krvarma/mutating-webhook</li> <li>https://trstringer.com/kubernetes-mutating-webhook/</li> <li>https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks</li> <li>https://github.com/kube-rs/kube/tree/main/examples#kube-admission-controller-example</li> </ul>","tags":["Kubernetes","extensible-admission-controllers"]},{"location":"Glossary/Kubernetes/cert-manager/","title":"Cert-manager","text":"","tags":["Kubernetes","CNCF","certificates","x509"]},{"location":"Glossary/Kubernetes/cert-manager/#installation","title":"Installation","text":"<p>To perform a default installation of cert-manager1 one can perform the following command:</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.10.1/cert-manager.yaml\n</code></pre>","tags":["Kubernetes","CNCF","certificates","x509"]},{"location":"Glossary/Kubernetes/cert-manager/#selfsigned-clusterissuer","title":"SelfSigned ClusterIssuer","text":"<p>In order to setup a self-signed cluster-global issuer, apply the following manifest:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\nname: selfsigned-issuer\nspec:\nselfSigned: {}\n</code></pre>","tags":["Kubernetes","CNCF","certificates","x509"]},{"location":"Glossary/Kubernetes/cert-manager/#selfsigned-ca-certificate","title":"SelfSigned CA Certificate","text":"<p>In order to setup a self-signed ca certificate, apply the following manifests:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\nname: nb-annotator\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\nname: my-selfsigned-ca\nnamespace: nb-annotator\nspec:\nisCA: true\ncommonName: my-selfsigned-ca\nsecretName: root-secret\nprivateKey:\nalgorithm: ECDSA\nsize: 256\nissuerRef:\nname: selfsigned-issuer\nkind: ClusterIssuer\ngroup: cert-manager.io\n---\napiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\nname: my-ca-issuer\nnamespace: nb-annotator\nspec:\nca:\nsecretName: root-secret\n</code></pre> <ol> <li> <p>https://cert-manager.io/docs/installation/ \u21a9</p> </li> </ol>","tags":["Kubernetes","CNCF","certificates","x509"]},{"location":"Glossary/Kubernetes/k3d/","title":"k3d","text":"<p>k3d is a lightweight wrapper to run\u00a0k3s\u00a0(Rancher Lab\u2019s minimal Kubernetes distribution) in docker. k3d makes it very easy to create single- and multi-node\u00a0k3s\u00a0clusters in docker, e.g. for local development on Kubernetes.1</p>","tags":["Kubernetes","development-environment"]},{"location":"Glossary/Kubernetes/k3d/#installation","title":"Installation","text":"<p>For installation options, refer to the official k3d documentation: https://k3d.io/v5.4.6/#installation.</p>","tags":["Kubernetes","development-environment"]},{"location":"Glossary/Kubernetes/k3d/#usage","title":"Usage","text":"","tags":["Kubernetes","development-environment"]},{"location":"Glossary/Kubernetes/k3d/#registry-creation","title":"Registry Creation","text":"<p>Create a local registry by using the following:</p> <pre><code>k3d registry create default-registry.localhost --port 9090\n</code></pre>","tags":["Kubernetes","development-environment"]},{"location":"Glossary/Kubernetes/k3d/#cluster-creation","title":"Cluster Creation","text":"<p>Create a local development cluster consisting of 3 nodes using the following:</p> <pre><code>k3d cluster create default --servers 3 --registry-use k3d-default-registry.localhost:9090\n</code></pre> <p>This requires changes made to the local hosts file2.</p>","tags":["Kubernetes","development-environment"]},{"location":"Glossary/Kubernetes/k3d/#teardown","title":"Teardown","text":"<p>Delete a local development cluster by running:</p> <pre><code>k3d cluster delete default\nk3d registry delete default-registry.localhost\n</code></pre> <ol> <li> <p>https://k3d.io/v5.4.6/ \u21a9</p> </li> <li> <p>https://k3d.io/v5.2.1/usage/registries/#preface-referencing-local-registries \u21a9</p> </li> </ol>","tags":["Kubernetes","development-environment"]},{"location":"Glossary/Kubernetes/CNI/CNI%20Bandwidth%20Limiting/","title":"CNI Bandwidth Limiting","text":"","tags":["Kubernetes","Cilium","CNI"]},{"location":"Glossary/Kubernetes/CNI/CNI%20Bandwidth%20Limiting/#related-to","title":"Related To","text":"<ul> <li>Network Bandwidth Limits</li> </ul>","tags":["Kubernetes","Cilium","CNI"]},{"location":"Glossary/Kubernetes/CNI/CNI%20Bandwidth%20Limiting/#resources","title":"Resources","text":"<ul> <li>https://isovalent.com/blog/post/addressing-bandwidth-exhaustion-with-cilium-bandwidth-manager/</li> <li>https://docs.cilium.io/en/v1.9/gettingstarted/bandwidth-manager/</li> <li>https://tcpcc.systemsapproach.org/index.html</li> <li>https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping</li> <li>https://www.cni.dev/plugins/current/meta/bandwidth/</li> </ul>","tags":["Kubernetes","Cilium","CNI"]},{"location":"Glossary/Kubernetes/CNI/How%20Kubernetes%20Networking%20Works/","title":"How Kubernetes Networking Works","text":"","tags":["Kubernetes","networking","CNI"]},{"location":"Glossary/Kubernetes/CNI/How%20Kubernetes%20Networking%20Works/#resources","title":"Resources","text":"<ul> <li>https://learnk8s.io/kubernetes-network-packets</li> </ul>","tags":["Kubernetes","networking","CNI"]},{"location":"Glossary/Kubernetes/CRI/Runtime%20Metrics/","title":"Runtime Metrics","text":"","tags":["Kubernetes","CRI","metrics"]},{"location":"Glossary/Kubernetes/CRI/Runtime%20Metrics/#resources","title":"Resources","text":"<ul> <li>https://docs.docker.com/config/containers/runmetrics/#interface-level-counters</li> <li>https://github.com/lensapp/lens/blob/master/src/main/prometheus/stacklight-provider.injectable.ts#L91</li> <li>https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md</li> <li>https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-3-container-resource-metrics-361c5ee46e66</li> <li>https://prometheus.io/docs/guides/cadvisor/#other-expressions</li> </ul>","tags":["Kubernetes","CRI","metrics"]},{"location":"Glossary/Kubernetes/Scheduler/Kubernetes%20Scheduler/","title":"Kubernetes Scheduler","text":"","tags":["Kubernetes","scheduler","kube-scheduler"]},{"location":"Glossary/Kubernetes/Scheduler/Kubernetes%20Scheduler/#resources","title":"Resources","text":"<ul> <li>https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/</li> <li>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/</li> <li>https://kubernetes.io/docs/concepts/scheduling-eviction/resource-bin-packing/</li> <li>https://github.com/kubernetes-sigs/scheduler-plugins/tree/master/kep/61-Trimaran-real-load-aware-scheduling</li> </ul>","tags":["Kubernetes","scheduler","kube-scheduler"]},{"location":"Glossary/Kubernetes/Scheduler/Load%20Aware%20Scheduling/","title":"Load Aware Scheduling","text":"<p>In 2020, a KEP proposed introducing a \"Real Load Aware Scheduling\" scheduler plugin called \"Trimaran\" to the Kubernetes' scheduler plugins repo.1</p> <p>In its first and current draft, it focuses on providing CPU load-aware scheduling and does not focus on \"Memory, Network, and Disk utilization[...]\" 2</p> <ol> <li> <p>https://github.com/kubernetes-sigs/scheduler-plugins/tree/master/kep/61-Trimaran-real-load-aware-scheduling \u21a9</p> </li> <li> <p>https://github.com/kubernetes-sigs/scheduler-plugins/tree/master/kep/61-Trimaran-real-load-aware-scheduling#non-goals \u21a9</p> </li> </ol>"},{"location":"Glossary/Kubernetes/Scheduler/Topology%20Aware%20Scheduling/","title":"Topology Aware Scheduling","text":"<p>In 2020 there was a proposal and implementation of a topology-aware scheduler.</p> <ul> <li>https://github.com/kubernetes-sigs/scheduler-plugins/blob/master/kep/119-node-resource-topology-aware-scheduling/README.md</li> <li>https://docs.google.com/document/d/12kj3fK8boNuPNqob6F_pPU9ZTaNEnPGaXEooW1Cilwg/edit</li> </ul>","tags":["Kubernetes","kube-scheduler"]},{"location":"Network%20Topology%20Aware%20Scheduler/","title":"Network Topology Aware Scheduler","text":"","tags":["Kubernetes","kube-scheduler","scheduler"]},{"location":"Network%20Topology%20Aware%20Scheduler/#node-annotations","title":"Node Annotations","text":"<ul> <li><code>node.kubernetes.io/network-limit: 1Gbps</code> - Indicates the maximum network bandwidth of a node.</li> </ul> <p>Potentially also add the following annotations:</p> <ul> <li><code>node.kubernetes.io/network-baseline: 1.25Gbps</code> - Indicates a node's guaranteed network bandwidth baseline</li> <li><code>node.kubernetes.io/network-burst: 10Gbps</code> - Indicates a node's potential burst bandwidth limit</li> </ul>","tags":["Kubernetes","kube-scheduler","scheduler"]},{"location":"Network%20Topology%20Aware%20Scheduler/#scheduling-using-extended-resources","title":"Scheduling Using Extended Resources","text":"<p>One can perform static scheduling with network-topology-related metrics by employing the steps outlined in Scheduling Using Extended Resources.</p> <p>While static scheduling is feasible to achieve that way, multiple shortcomings become apparent.</p> <ul> <li>One has to implement a controller updating each node's bandwidth resource capacities and allocatable amounts.</li> <li>As outlined in the pod definition section, the CNI specification does not consider custom resource limits. It would require creating an additional mutating webhook that intercepts the creation of a pod resource and sets the annotations accordingly.</li> </ul> <p>Additionally, by design Scheduling Using Extended Resources does not allow overcommitting resources and thus cannot take bursts into account, unless specific modification to the pod objects occur using mutating webhooks.  In theory, pods could still be scheduled on nodes with burst capacity, yet as extended resources are incompatible with burst, those available burst credits are left unused.</p>","tags":["Kubernetes","kube-scheduler","scheduler"]},{"location":"Network%20Topology%20Aware%20Scheduler/Network%20Bandwidth%20Annotation%20Manager/","title":"Network Bandwidth Annotation Manager","text":"<p>As part of this thesis and to create a generic way of handling networking-related resource requests, I made a small mutating webhook admission controller for Kubernetes.</p> <p>The Network Bandwidth Annotation Manager, abbreviated to <code>nbam</code>, is an admission webhook that enables one to alter pod objects before the apiserver persists them to its storage, thus before any scheduling has taken place.</p> <p>The primary motivation behind creating nbam is the ability to use extended resource FQDNs in pod resource requests, as many helm charts or other packaged Kubernetes deployments do not allow setting custom pod annotations, as required by the CNI spec. Yet, one can usually set CPU and memory limits in helm charts or Kubernetes primitives. Thus nbam takes care of rewriting those to the corresponding pod annotations in multiple modes.</p>","tags":["Kubernetes","extensible-admission-controllers"]},{"location":"Network%20Topology%20Aware%20Scheduler/Network%20Bandwidth%20Annotation%20Manager/#setup","title":"Setup","text":"<ul> <li>Setup a local Kubernetes cluster using k3d</li> <li>Install cert-manager.</li> <li>Deploy the annotation manager using this manifest.</li> <li>Add network-related resources to nodes as outlined in Scheduling Using Extended Resources#Adding Network Related Resources To Nodes</li> <li>Deploy examples located at: https://github.com/ThomasK33/network-bandwidth-annotation-manager/blob/main/examples</li> </ul>","tags":["Kubernetes","extensible-admission-controllers"]},{"location":"Network%20Topology%20Aware%20Scheduler/Scheduling%20Using%20Extended%20Resources/","title":"Scheduling Using Extended Resources","text":"<p>Extended resources are fully-qualified resource names outside the\u00a0<code>kubernetes.io</code>\u00a0domain. They allow cluster operators to advertise and users to consume the non-Kubernetes-built-in resources. There are two steps required to use Extended Resources. First, the cluster operator must advertise an Extended Resource. Second, users must request the Extended Resource in Pods.1</p> <p>One can add networking-related information to a node's ingress and egress capacities and allocatable amounts by utilizing Extended Resources.</p> <p>Those can either be added manually, by a controller or using a device plugin.6</p>","tags":["Kubernetes","scheduler","kube-scheduler"]},{"location":"Network%20Topology%20Aware%20Scheduler/Scheduling%20Using%20Extended%20Resources/#shortcomings","title":"Shortcomings","text":"<p>Note:\u00a0Extended resources cannot be overcommitted, so request and limit must be equal if both are present in a container spec.5</p>","tags":["Kubernetes","scheduler","kube-scheduler"]},{"location":"Network%20Topology%20Aware%20Scheduler/Scheduling%20Using%20Extended%20Resources/#adding-network-related-resources-to-nodes","title":"Adding Network Related Resources To Nodes","text":"<p>Create a new shell session, then proxy the api-server to a local port, with authentication already in place:</p> <pre><code>kubectl proxy\n</code></pre>","tags":["Kubernetes","scheduler","kube-scheduler"]},{"location":"Network%20Topology%20Aware%20Scheduler/Scheduling%20Using%20Extended%20Resources/#capacity-information","title":"Capacity Information","text":"<p>Set a node's status capacity information by using, e.g.:</p> <pre><code>curl --header \"Content-Type: application/json-patch+json\" \\\n--request PATCH \\\n--data '[{\"op\": \"add\", \"path\": \"/status/capacity/networking.k8s.io~1ingress-bandwidth\", \"value\": \"1.25e+9\"}]' \\\nhttp://localhost:8001/api/v1/nodes/k3d-default-server-0/status\n\ncurl --header \"Content-Type: application/json-patch+json\" \\\n--request PATCH \\\n--data '[{\"op\": \"add\", \"path\": \"/status/capacity/networking.k8s.io~1egress-bandwidth\", \"value\": \"1.25e+9\"}]' \\\nhttp://localhost:8001/api/v1/nodes/k3d-default-server-0/status\n</code></pre> <p>(Note: <code>1.25e+9</code> is equivalent to <code>1.25Gbps</code>.)</p> <p>Applied to a local as outlined in k3d#Cluster Creation:</p> <pre><code>curl --header \"Content-Type: application/json-patch+json\" \\\n--request PATCH \\\n--data '[{\"op\": \"add\", \"path\": \"/status/capacity/networking.k8s.io~1ingress-bandwidth\", \"value\": \"1.25e+9\"}]' \\\nhttp://localhost:8001/api/v1/nodes/k3d-default-server-0/status\n\ncurl --header \"Content-Type: application/json-patch+json\" \\\n--request PATCH \\\n--data '[{\"op\": \"add\", \"path\": \"/status/capacity/networking.k8s.io~1egress-bandwidth\", \"value\": \"1.25e+9\"}]' \\\nhttp://localhost:8001/api/v1/nodes/k3d-default-server-0/status\n\ncurl --header \"Content-Type: application/json-patch+json\" \\\n--request PATCH \\\n--data '[{\"op\": \"add\", \"path\": \"/status/capacity/networking.k8s.io~1ingress-bandwidth\", \"value\": \"1.25e+9\"}]' \\\nhttp://localhost:8001/api/v1/nodes/k3d-default-server-1/status\n\ncurl --header \"Content-Type: application/json-patch+json\" \\\n--request PATCH \\\n--data '[{\"op\": \"add\", \"path\": \"/status/capacity/networking.k8s.io~1egress-bandwidth\", \"value\": \"1.25e+9\"}]' \\\nhttp://localhost:8001/api/v1/nodes/k3d-default-server-1/status\n\ncurl --header \"Content-Type: application/json-patch+json\" \\\n--request PATCH \\\n--data '[{\"op\": \"add\", \"path\": \"/status/capacity/networking.k8s.io~1ingress-bandwidth\", \"value\": \"1.25e+9\"}]' \\\nhttp://localhost:8001/api/v1/nodes/k3d-default-server-2/status\n\ncurl --header \"Content-Type: application/json-patch+json\" \\\n--request PATCH \\\n--data '[{\"op\": \"add\", \"path\": \"/status/capacity/networking.k8s.io~1egress-bandwidth\", \"value\": \"1.25e+9\"}]' \\\nhttp://localhost:8001/api/v1/nodes/k3d-default-server-2/status\n</code></pre>","tags":["Kubernetes","scheduler","kube-scheduler"]},{"location":"Network%20Topology%20Aware%20Scheduler/Scheduling%20Using%20Extended%20Resources/#allocatable-amount-information","title":"Allocatable Amount Information","text":"<p>Furthermore, if needed, one could also set a lower allocatable amount:</p> <pre><code>curl --header \"Content-Type: application/json-patch+json\" \\\n--request PATCH \\\n--data '[{\"op\": \"add\", \"path\": \"/status/allocatable/networking.k8s.io~1ingress-bandwidth\", \"value\": \"1e+9\"}]' \\\nhttp://localhost:8001/api/v1/nodes/k3d-default-server-0/status\n\ncurl --header \"Content-Type: application/json-patch+json\" \\\n--request PATCH \\\n--data '[{\"op\": \"add\", \"path\": \"/status/allocatable/networking.k8s.io~1egress-bandwidth\", \"value\": \"1e+9\"}]' \\\nhttp://localhost:8001/api/v1/nodes/k3d-default-server-0/status\n</code></pre>","tags":["Kubernetes","scheduler","kube-scheduler"]},{"location":"Network%20Topology%20Aware%20Scheduler/Scheduling%20Using%20Extended%20Resources/#using-resources-in-pod-definition","title":"Using Resources In Pod Definition","text":"<p>For traffic shaping2, Pods require an annotation for ingress3 and egress4 bandwidth limits and do not utilize the resource specs. The kube-scheduler only uses the resource request specification for scheduling. Therefore, it ignores the annotations, while the CNI Bandwidth Limiting ignores the specified resource requests and only utilizes the annotations for limiting.</p> <p>In that sense, setting container limits in <code>spec.containers[*].resources.limits.ingress-bandwidth</code> or <code>spec.containers[*].resources.limits.egress-bandwidth</code> is pointless unless a mutating webhook rewrites the pod definition and sets the required pod's bandwidth annotations based on the limits if no bandwidth related annotations are present.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: my-pod\nannotations:\n# This is necessary because of the way CNI traffic shaping support\n# currently implements its limits\nkubernetes.io/ingress-bandwidth: 1M\nkubernetes.io/egress-bandwidth: 1M\nspec:\ncontainers:\n- name: my-container\nimage: myimage\nresources:\nrequests:\ncpu: 2\n# if the pod contains ingress and egress bandwidth annotations\n# the requests will be automatically set to the annotations values\nnetworking.k8s.io/ingress-bandwidth: 1M\nnetworking.k8s.io/egress-bandwidth: 1M\nlimits:\ncpu: 4\n# Not used by neither the kube-scheduler, nor the CNI\nnetworking.k8s.io/ingress-bandwidth: 1M\nnetworking.k8s.io/egress-bandwidth: 1M\n</code></pre> <ol> <li> <p>https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#extended-resources \u21a9</p> </li> <li> <p>https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping \u21a9</p> </li> <li> <p>https://kubernetes.io/docs/reference/labels-annotations-taints/#kubernetes-io-ingress-bandwidth \u21a9</p> </li> <li> <p>https://kubernetes.io/docs/reference/labels-annotations-taints/#kubernetes-io-egress-bandwidth \u21a9</p> </li> <li> <p>https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#consuming-extended-resources \u21a9</p> </li> <li> <p>https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/ \u21a9</p> </li> </ol>","tags":["Kubernetes","scheduler","kube-scheduler"]},{"location":"Glossary/tags/","title":"Tags","text":""},{"location":"Glossary/tags/#aws","title":"AWS","text":"<ul> <li>Network Bandwidth Limits</li> </ul>"},{"location":"Glossary/tags/#cncf","title":"CNCF","text":"<ul> <li>Cert-manager</li> </ul>"},{"location":"Glossary/tags/#cni","title":"CNI","text":"<ul> <li>CNI Bandwidth Limiting</li> <li>How Kubernetes Networking Works</li> </ul>"},{"location":"Glossary/tags/#cri","title":"CRI","text":"<ul> <li>Runtime Metrics</li> </ul>"},{"location":"Glossary/tags/#cilium","title":"Cilium","text":"<ul> <li>CNI Bandwidth Limiting</li> </ul>"},{"location":"Glossary/tags/#ec2","title":"EC2","text":"<ul> <li>Network Bandwidth Limits</li> </ul>"},{"location":"Glossary/tags/#kubernetes","title":"Kubernetes","text":"<ul> <li>Bachelor Thesis</li> <li>Custom Resources</li> <li>Metrics Server</li> <li>Mutating Webhooks</li> <li>Cert-manager</li> <li>k3d</li> <li>CNI Bandwidth Limiting</li> <li>How Kubernetes Networking Works</li> <li>Runtime Metrics</li> <li>Kubernetes Scheduler</li> <li>Topology Aware Scheduling</li> <li>Network Topology Aware Scheduler</li> <li>Network Bandwidth Annotation Manager</li> <li>Scheduling Using Extended Resources</li> </ul>"},{"location":"Glossary/tags/#autoscaling","title":"autoscaling","text":"<ul> <li>Metrics Server</li> </ul>"},{"location":"Glossary/tags/#certificates","title":"certificates","text":"<ul> <li>Cert-manager</li> </ul>"},{"location":"Glossary/tags/#development-environment","title":"development-environment","text":"<ul> <li>k3d</li> </ul>"},{"location":"Glossary/tags/#extensible-admission-controllers","title":"extensible-admission-controllers","text":"<ul> <li>Mutating Webhooks</li> <li>Network Bandwidth Annotation Manager</li> </ul>"},{"location":"Glossary/tags/#kube-scheduler","title":"kube-scheduler","text":"<ul> <li>Kubernetes Scheduler</li> <li>Topology Aware Scheduling</li> <li>Network Topology Aware Scheduler</li> <li>Scheduling Using Extended Resources</li> </ul>"},{"location":"Glossary/tags/#metrics","title":"metrics","text":"<ul> <li>Metrics Server</li> <li>Runtime Metrics</li> </ul>"},{"location":"Glossary/tags/#networking","title":"networking","text":"<ul> <li>Network Bandwidth Limits</li> <li>How Kubernetes Networking Works</li> </ul>"},{"location":"Glossary/tags/#scheduler","title":"scheduler","text":"<ul> <li>Kubernetes Scheduler</li> <li>Network Topology Aware Scheduler</li> <li>Scheduling Using Extended Resources</li> </ul>"},{"location":"Glossary/tags/#thesis","title":"thesis","text":"<ul> <li>Bachelor Thesis</li> </ul>"},{"location":"Glossary/tags/#uni","title":"uni","text":"<ul> <li>Bachelor Thesis</li> </ul>"},{"location":"Glossary/tags/#x509","title":"x509","text":"<ul> <li>Cert-manager</li> </ul>"}]}